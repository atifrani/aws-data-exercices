# Exercise 02

This lab will give you an understanding of the AWS Glue â€“ a fully managed data catalog and ETL service.  

## What is ETL:  
In the world of data warehousing, if you need to bring data from multiple different data sources into one, centralized database, you must first:  

    * EXTRACT data from its original source  
    * TRANSFORM data by deduplicating it, combining it, and ensuring quality, to then  
    * LOAD data into the target database  
The ETL process is comprised of these 3 steps that enable data integration from source to destination: data Extraction, data Transformation, and data Loading.  

## what is aws Glue:  
AWS Glue is a serverless data integration service that makes it easy for analytics users to discover, prepare, move, and integrate data from multiple sources. You can use it for analytics, machine learning, and application development. It also includes additional productivity and data ops tooling for authoring, running jobs, and implementing business workflows. 

With AWS Glue, you can discover and connect to more than 70 diverse data sources and manage your data in a centralized data catalog. You can visually create, run, and monitor extract, transform, and load (ETL) pipelines to load data into your data lakes.  

## PreLab setup:

AWS s3 busket is used as a source of sales system retaildb. It stores transaction information about categories, customers, departements, order_items, orders and products.  
Before the Glue lab starts, check that the bucket is already created and that the data is in the bucket, otherwise, create the bucket and copy the source data to your S3 bucket.  
In today's lab, you will copy the data from a centralized S3 bucket to your AWS account,
crawl the dataset with AWS Glue crawler for metadata creation and transform the data
with AWS Glue to Query data and create a View with Athena and Build a dashboard with
Amazon QuickSight.  

## Copy Data across from staging Amazon S3 bucket to your S3 bucket

Issue the following commands in the terminal, and replace the bucket name with your own one.  

aws s3 cp --recursive s3://axelt-retail-db/categories/  s3://<YourBucketName>/categories/  
aws s3 cp --recursive s3://axelt-retail-db/customers/  s3://<YourBucketName>/customers/  
aws s3 cp --recursive s3://axelt-retail-db/departments/  s3://<YourBucketName>/departments/  
aws s3 cp --recursive s3://axelt-retail-db/order_items/  s3://<YourBucketName>/order_items/  
aws s3 cp --recursive s3://axelt-retail-db/orders/  s3://<YourBucketName>/orders/  
aws s3 cp --recursive s3://axelt-retail-db/products/  s3://<YourBucketName>/products/  

## Verify the Data:
  
1. Open the S3 console and view the data that was copied through terminal.  
2. Your S3 bucket name will look like below :  
BucketName/bucket_folder_name/objects  
3. Navigate to one of the files and review it using S3 Select:  
    a. Navigate in to the directory named categories and select the check box next to the file name categories.csv.  
    b. Click the Actions dropdown button and choose Query with S3 Select.  
    c. In the Query with S3 Select page, leave the default value for Input Settings and SQL Query and
click Run SQL query.  
    d. It will execute the specified SQL query and return the first 5 lines from the CSV file.  
Explore the objects in the S3 directory further.  

## PART A: Data Validation and ETL  
Create Glue Crawler for initial full load data  
1. Navigate to the AWS Glue service  
2. On the AWS Glue menu, select Crawlers.  
3. in Data catalog section, click dd crawlers.  
4. Click Create crawler.  
5. Enter retail-lab-crawler as the crawler name for initial data load.  
6. Optionally, enter the description. This should also be descriptive and easily recognized and Click
Next.  

![glue crawler](data/images/glue1.png)