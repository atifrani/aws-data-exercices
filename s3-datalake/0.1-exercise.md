# Exercise 01

* Create s3 bucket:
  aws s3 mb s3://at-retail-db --region eu-west-1  
  aws s3 ls  
  aws s3 ls at-retail-db

* Copy retail-db subfolders to s3:
  
  aws s" cp data/retail-db/categories  s3://at-retail-db --recursive  
  aws s3 ls at-retail-db  
  aws s3 ls at-retail-db/categories  

  aws s3 cp data/retail-db/customers  s3://at-retail-db --recursive  
  aws s3 ls at-retail-db  
  aws s3 ls at-retail-db/customers  

  aws s" cp data/retail-db/departments  s3://at-retail-db --recursive  
  aws s3 ls at-retail-db  
  aws s3 ls at-retail-db/departments  

  aws s3 cp data/retail-db/order_items  s3://at-retail-db --recursive  
  aws s3 ls at-retail-db  
  aws s3 ls at-retail-db/order_items  

  aws s3 cp data/retail-db/orders  s3://at-retail-db --recursive  
  aws s3 ls at-retail-db  
  aws s3 ls at-retail-db/orders  

  aws s3 cp data/retail-db/products  s3://at-retail-db --recursive  
  aws s3 ls at-retail-db  
  aws s3 ls at-retail-db/products  

after you finish copying the retail-db data to s3, connect to you RDS postgresql database using dbeaver.

* Create retail-db database schema
  create schema retaildb;

* Create retaildb tables:
  
  * create table categories:  
    create retaildb.categories  
    (  
        category_id integer not null,  
        departement_id integer,  
        category_name varchar  
    );  

  * create table customers:
    create retaildb.customers  
    (  
        customer_id integer not null,  
        customer_fname varchar,   
        customer_lname varchar,   
        customer_email varchar,  
        customer_password varchar,  
        customer_street varchar,  
        customer_city varchar,  
        customer_state varchar,  
        customer_zipcode varchar  
    );  

  * create table departments:
    create retaildb.departments  
    (  
        department_id integer not null,  
        departement_name varchar  
    );  

  * create table order_items:
    create retaildb.order_items  
    (  
        order_item_id integer not null,  
        order_item_order_id integer,  
        order_item_product_id integer,   
        order_item_quantity integer,  
        order_item_subtotal float,  
        order_item_price float  
    );  

  * create table orders:
    create retaildb.orders  
    (  
        order_id integer not null,  
        order_date date,  
        order_customer_id integer,  
        order_status varchar  
    );  

  * create table products:
    create retaildb.products  
    (  
        product_id integer not null,  
        product_category_id integer,  
        product_name varchar,
        product_description varchar,  
        product_price float,  
        product_image  varchar
    );  

* load at-retail-db bucket files into retaildb tables:
  * Install aws s3 extension:
    AWS RDS for PostgreSQL comes with an extension that allows you to fetch data from AWS S3 and to write back data to AWS S3. 
    * Check the available aws extensions:  
    Select * from pg_available_extensions where name like '%aws%'  

    If you try to install the extension you’ll notice that there is a dependency on the “aws_commons” extension:  
    create extension aws_s3;  

    You can install both extensions in one step using the “CASCADE” option:  
    create extension aws_s3 cascade;   

With Postgres we run the aws_s3.table_import_from_s3 command in order to import our S3 CSV file into Postgres.  
    
    select aws_s3.table_import_from_s3 (  
    'schema.table',  
    '',  
    '(format csv)',  
    'bucket-name',  
    'folder-name/file-name',  
    'region',  
    'aws-access-key',  
    'aws-secret-key'  
    );   

  *  Copy categories data:  

    select aws_s3.table_import_from_s3 (  
    'retaildb.categories',  
    '',  
    '(format csv)',  
    'at-retail-db',  
    'categories/categories.csv',  
    'eu-west-1',  
    '************',  
    '************'  
    );  

    Check if the data are loaded:  

    select * from retaildb.categories;  


  *  Copy customers data:  

    select aws_s3.table_import_from_s3 (  
    'retaildb.customers',  
    '',  
    '(format csv)',  
    'at-retail-db',  
    'customers/customers.csv',  
    'eu-west-1',  
    '************',  
    '************'  
    );  

    Check if the data are loaded:  

    select * from retaildb.customers;  

  *  Copy departments data:  

    select aws_s3.table_import_from_s3 (  
    'retaildb.departments',  
    '',  
    '(format csv)',  
    'at-retail-db',  
    'departments/departments.csv',  
    'eu-west-1',  
    '************',  
    '************'  
    );  

    Check if the data are loaded:  

    select * from retaildb.departments;  

  *  Copy order_items data:  

    select aws_s3.table_import_from_s3 (  
    'retaildb.order_items',  
    '',  
    '(format csv)',  
    'at-retail-db',  
    'order_items/order_items.csv',  
    'eu-west-1',  
    '************',  
    '************'  
    );  

    Check if the data are loaded:  

    select * from retaildb.order_items;  

  *  Copy orders data:  

    select aws_s3.table_import_from_s3 (  
    'retaildb.orders',  
    '',  
    '(format csv)',  
    'at-retail-db',  
    'orders/orders.csv',  
    'eu-west-1',  
    '************',  
    '************'  
    );  

    Check if the data are loaded:   

    select * from retaildb.orders;  

  *  Copy products data:  

    select aws_s3.table_import_from_s3 (  
    'retaildb.products',  
    '',  
    '(format csv)',  
    'at-retail-db',  
    'products/products.csv',  
    'eu-west-1',  
    '************',  
    '************'  
    );  

    Check if the data are loaded:   

    select * from retaildb.products; 


Now, let's create a new tables/datamarts:  
    
   *  Create table  top_product (product_id, product_name, category_name, month, year, sales )  
   *  Create table  top_customer (customer_id, customer_name, customer_street, customer_city,customer_zipcode, month, year, sales )  
  
After creating the 2 tables, export the data from theses tables to at-retail-db bucket.  

With Postgres we run the aws_s3.query_export_to_s3 command in order to export data from Postgres to aws S3.   
    
    select aws_s3.query_export_to_s3 (  
    'select * from schema.table',  
    'bucket-name',  
    'folder-name/file-name',  
    'region',  
    'aws-access-key',  
    'aws-secret-key'  
    );   

  *   Export top_product table:  

    select aws_s3.query_export_to_s3 (  
    'select * from retaildb.top_product',  
    'at-retail-db',  
    'top_product/top_product.csv',  
    'eu-west-1',  
    '***********',  
    '***********'  
    );  

  *   Export top_customer table:  

    select aws_s3.query_export_to_s3 (  
    'select * from retaildb.top_customer',  
    'at-retail-db',  
    'top_customer/top_customer.csv',  
    'eu-west-1',  
    '***********',  
    '***********'  
    );  